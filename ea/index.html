<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  
  
  
  
  
  <link rel="prev" href="https://ssaraf.com/bandwidth/" />
  
  <script defer data-domain="ssaraf.com" src="https://plausible.io/js/plausible.outbound-links.js"></script>
  <link rel="canonical" href="https://ssaraf.com/ea/" />
  <link rel='shortcut icon' type='image/x-icon' href='/favicon.ico' />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <meta name="google-site-verification" content="QasNAmwQUjj9XauzqCRuxf-gTK-e2ujgo4c3fwvDRX4" />
  <title>
       
       
           Why I disagree with effective altruism | ssaraf
       
  </title>
  <meta name="title" content="Why I disagree with effective altruism | ssaraf">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Why I disagree with effective altruism"/>
<meta name="twitter:description" content="Effective altruism is a &ldquo;research field, which aims to identify the world’s most pressing problems and the best solutions to them, and a practical community that aims to use those findings to do good.&rdquo; [1]
I&rsquo;ve long felt that EA was a bit silly, at least for a normal person. But I didn&rsquo;t have a good framework for why I felt that way.
I recently went on a walk with a friend involved in EA, and I thought the discussion was extremely thought-provoking (in a literal sense - my thoughts were provoked!"/>

  <script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "Why I disagree with effective altruism",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https:\/\/ssaraf.com\/ea\/"
  },
  
  "genre": "posts",
  "keywords": "writing",
  "wordcount":  2949 ,
  "url": "https:\/\/ssaraf.com\/ea\/",
  "datePublished": "2022-08-31T00:00:00\u002b00:00",
  "dateModified": "2022-08-31T00:00:00\u002b00:00",
  
  
  "author": {
    "@type": "Person",
    "name": "Sanjay Saraf"
  },
  "description": ""
}
</script>
</head>

  



  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="https://ssaraf.com">ssaraf</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/about" title="">About</a>
                
                <a class="menu-item" href="/posts/" title="">Writing</a>
                
                <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-sun"></i></a>&nbsp;
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-sun"></i></a>&nbsp;<a href="https://ssaraf.com">ssaraf</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/about" title="">About</a>
                
                <a class="menu-item" href="/posts/" title="">Writing</a>
                
        </div>
    </div>
</nav>

    	 <main class="main">
          <div class="container">
      		
<article class="post-warp">
    <header class="post-header">
        <h1 class="post-title">Why I disagree with effective altruism</h1>
        <div class="post-meta">
            Written by <a href="https://ssaraf.com" rel="author">Sanjay Saraf</a>  
                <span class="post-time">
                    in <time datetime=August&#32;2022 >August 2022</time>
                </span>
                 
        </div>
    </header>
    <div class="post-content">
        

        
            
        

        
        
     
          
          
          

          
          
          

          <p><strong>Effective altruism</strong> is a &ldquo;research field, which aims to identify the world’s most pressing problems and the best solutions to them, and a practical community that aims to use those findings to do good.&rdquo; [1]</p>
<p>I&rsquo;ve long felt that EA was a bit silly, at least for a normal person. But I didn&rsquo;t have a good framework for why I felt that way.</p>
<p>I recently went on a walk with a friend involved in EA, and I thought the discussion was extremely thought-provoking (in a literal sense - my thoughts were provoked!). It helped elucidate to me what felt wrong about the movement and its philosophy, and I thought that in classic EA spirit I&rsquo;d write it down and solicit some feedback. [2]</p>
<h2 id="bow-to-the-fashionistas">Bow to the fashionistas.</h2>
<p>First I should say, as all who criticize EA must, that EAs are wonderful people, that I have tremendous respect for what they stand for. I say that because I have to say it.</p>
<p>Because that is the first part of the problem. EA is <em>so</em> popular. Almost every intellectual I know either benignly or actively subscribes to a subset of the EA philosophy. Rich, famous people fund Open Philanthropy and GiveWell. I&rsquo;m just a regular shmoe, what <em>right</em> do I have to criticize what the world thinks is correct and good and moral?</p>
<p>Paul Graham has a nice piece about fashions, though fortunately for him he&rsquo;s now the most fashionable person in the valley [3]. As Paul describes, the problem with fashions is that lots of people follow them, and even if the fashion being followed is nominally self-reflective, it may miss a glaring, obvious mistake that&rsquo;s only visible in retrospect. EA&rsquo;s popularity should be a cause for suspicion, not a celebration.</p>
<p>More practically, for those who disagree or don&rsquo;t follow a fashion, the way to deal with it is to <em>not engage</em>. What I am writing this for, exactly? The best case scenario is that almost no one reads it. The worst case is it gets popular, and the mob comes after me.</p>
<p>This isn&rsquo;t a direct criticism of EA, of course. But it means that real criticisms of EA are either unlikely to be written or, maybe more frighteningly, not as likely to be thought about. People will <em>assume</em> that EA is the correct moral philosophy, rather than litigating it fully as I believe it deserves. Perhaps that&rsquo;s part of the reason why many critiques of EA tend to be of the flavor &ldquo;assume EA is right, here&rsquo;s a specific thing they do that I disagree with&rdquo; rather than &ldquo;EA is wrong, here&rsquo;s why&rdquo;.</p>
<p>Just to provide a tangible example here, it looks like Scott Alexander has strong opinions about EA, but isn&rsquo;t publishing them because they&rsquo;re too spicy. [4] He&rsquo;s SSC! It should be worrying that someone so popular and prominent among &ldquo;rationalists&rdquo; is worried that his hot take might be too hot.</p>
<h2 id="come-all-ye-faithful">Come all ye faithful</h2>
<p>EAs like to think of themselves as rational, rigorous, and quantitative. They conduct and read studies about what donations are most effective. They are, at least in their minds, Richard Feynmans, except with altruistic endeavors instead of physics.</p>
<p>But something doesn&rsquo;t seem quite right. For example, if you ask an EA how much you should donate a year, they suggest roughly 10% of your income, regardless of how much you earn.</p>
<p>Does that number sound familiar? It sounds a little bit like a tithe. Where&rsquo;s the quantification?</p>
<p>Here&rsquo;s what I would expect from a true &ldquo;effective altruist&rdquo;: I&rsquo;d expect them to do a rough analysis of their existing audience - perhaps western hemisphere intellectuals of the upper middle class. Then they&rsquo;d generate a histogram of typical incomes of people in the EA movement. Then they&rsquo;d plot that vs incremental marginal utility. And I&rsquo;d expect them to suggest a percentage of income that changes depending on that marginal utility calculation.</p>
<p>After all, EAs have gone through the trouble of quantifying the suffering of a chicken in a cage for an expected value calculation. [5] Why haven&rsquo;t they done rigorous quantification of the input to a charity, not just the output?</p>
<p>Because EA sometimes looks a lot more like a religion than an intellectual movement.</p>
<p>I grew up in a small town in Louisiana, a very religious place. I&rsquo;m used to people trying to convert me to their faith. I see the same elements in many EAs.</p>
<p>There is the worship of it, the acceptance of all the core tenets, and the debates over clerical details. There is an assumption that they alone know the truth, and those who disagree should be evangelized to and brought into the fold. Prominent ideas, which on the surface seem to disagree with their philosophy get subsumed or merged into EA, like Saturnalia becoming Christmas.</p>
<p>The main difference is that EAs <em>worship</em> quantification. They worship correctness, at least in some sense. Which makes it a little bit difficult to criticize them, in what Michael Neilson describes as &ldquo;EA judo&rdquo; - many criticisms of EA turn into something that makes EA stronger. [6]</p>
<p>For example, one group of EAs tends to prioritize short-term interventions that have quantifiable amounts of utility. Another group of EAs tend to prioritize long-term interventions which have unknown utility (perhaps they mediate the future risk of the apocalypse by a small, unknown, percentage). You might <em>expect</em> that an effective altruist organization would have a paradigm for how to prioritize between these two. Isn&rsquo;t the whole point that we measure the utility of any intervention, compare them, and then make a <em>choice</em>?</p>
<p>No&hellip;remember the EA philosophy - anything you see that isn&rsquo;t EA, seen from another angle, can become EA. Can&rsquo;t quantify the impact of a long-term intervention? That&rsquo;s okay - it still might be effective!</p>
<p>Open Philanthropy, a very prominent effective altruism organization, actually has two leaders, one who focuses on short-term quantitative interventions, and the other focuses on long-term abstract ones.</p>
<p>I&rsquo;m not saying that&rsquo;s the wrong choice. But I think I have gripes with describing this as effective altruism. EA doesn&rsquo;t seem to be that useful of a paradigm if you&rsquo;re willing to mold it and reshape it this flexibly. It doesn&rsquo;t seem like there&rsquo;s a good rationale for having both of these classes of investment, and if you can&rsquo;t distinguish which one is better, how do I know which to contribute to?</p>
<h2 id="the-goal-is-not-to-maximize-quantifiable-utility">The goal is not to maximize quantifiable utility</h2>
<p>Initially, EA&rsquo;s goal appeared to be a version of utilitarianism. Measure what good you can do in utils for intervention A, compare it to interventions B, C, and D, and put money into the causes in decreasing order of maximal utility.</p>
<p>But most EA organizations don&rsquo;t do this exactly. They hedge by putting money into the top 10 interventions, even if more money could go to the top 1 by depriving the other 9.</p>
<p>This is a small sin but I find it to be significant. EAs have a lot of capital, but they don&rsquo;t seem to deploy it in a way that shows they believe in their effectiveness calculations. If you believe that A is strictly better than B, why in the world are you funding B? It doesn&rsquo;t appear to be the case, for example, that cause A is oversubscribed or has achieved diminishing returns.</p>
<p>I could nitpick at a hundred of these types of things. For example, &ldquo;EA marketing&rdquo; is also a cause on the list of things EA organizations implicitly fund, but I&rsquo;m not sure I&rsquo;ve seen the calculation of how many utils their marketing is worth relative to buying more bed nets.</p>
<p><strong>The sheer <em>number</em> of causes that most prominent EA organizations pursue (whether they call them causes or not) means that a &ldquo;utility-based&rdquo; analysis does not appear to explain their actions.</strong></p>
<p>Instead, at least some EAs see certain interventions as having uncertainty and risk. Holden Karnofsky writes about some of that &ldquo;hits-based giving&rdquo;&hellip; there are interventions, many of them, where you are not certain about the exact amount of value it will have, but it is possible that by investing in a portfolio of such interventions, one or two will make up for the rest in terms of value provided. [7]</p>
<p>That leads to EAs funding all sorts of (in my opinion) kooky ideas. For example, animal welfare is funded at 3x the amount that climate change is. [8]</p>
<p><figure><img src="/images/ring.svg" data-sizes="auto" data-src="/ea_cause_pri.png" alt="" class="lazyload"><figcaption class="image-caption"></figcaption></figure></p>
<p>The problem is that debating between, say, animal welfare and climate change is it&rsquo;s no longer a quantitative analysis of what provides the most good. Instead, we have a qualitative analysis of something like &ldquo;if animals are beings with consciousness, the value of preventing their suffering would be high, thus even if the aforementioned is unlikely we will fund this to maximize expected utility&rdquo;. We are not investing in the best charities, we are portfolio managers investing in a bunch of things where at least one might hit it out of the park, diversifying our risk away.</p>
<p>I actually think that rationale is pretty reasonable, but it makes it hard to reason about which things deserve a &ldquo;judgment call&rdquo; vs which things are measured directly in QALYs.</p>
<h2 id="uncertainty-and-error-bars-on-interventions">Uncertainty and error bars on interventions</h2>
<p>This brings me to my central issue with effective altruism. EAs are, with the left hand, the rigorous quantitative measurers they portray themselves to be. That means any short-term intervention is evaluated with objectivity. Food banks are a <em>mistake</em>. <em>Never</em> contribute to local radio. Think of the utils! Think of the 100x or 1000x multiple you can get by reducing global poverty with that incremental dollar!</p>
<p>And yet, on the right hand, they are qualitative estimators of distant future events, immeasurable risks, and unquantifiable value. They see the proto-souls of chickens and mice and pigs and say, per their Drake equation-like calculations, that they are more valuable than other interventions. [9]</p>
<p>And so, in aggregate, the population of EAs is spending their time on health in third world countries and animal welfare, while there is poop at their doorstep (literally - I live in San Francisco, remember). There is limited awareness of that cognitive dissonance.</p>
<p>Let me try to tease it apart a bit more.</p>
<h2 id="why-regular-people-are-not-likely-to-subscribe-to-ea">Why regular people are not likely to subscribe to EA</h2>
<p>It should be obvious that as income goes up, there is diminishing marginal utility in each extra dollar. Dustin Moskovitz can contribute a hundred grand, or even a million dollars, and hardly break a sweat.</p>
<p>But Joe Sixpack (yes he&rsquo;s back from 2008), who still has a mortgage, and a car note, really would appreciate that extra hundred bucks. I&rsquo;ve already discussed my general surprise that the EA community doesn&rsquo;t have a calculator where expected personal utility goes down as income goes up because it could make their donation recommendations far more specific and useful.</p>
<p>But how should we think about personal marginal utility vs global?</p>
<h2 id="i-care-about-myself-a-lot-more-than-i-care-about-an-anonymous-person">I care about myself a lot more than I care about an anonymous person.</h2>
<p>I know, it&rsquo;s selfish. But a hundred starving people in China won&rsquo;t have nearly as much impact on me as my nephew burning his hand on the stove. Personal utility matters a lot! None of us is God, looking upon each of our human subjects as children to be gardened and nurtured. We are biased.</p>
<p>I care about myself, my family, my community, my city, and my country. I don&rsquo;t think that&rsquo;s weird or some kind of weakness. Personal utility is not just a way to look at the world, it&rsquo;s <em>the</em> way that everyone (including, I suspect,  those who profess they are EA) truly lives.</p>
<p>Even those people who have gone above and beyond, donating kidneys, dedicating their entire working lives to helping people around the world&hellip;those people still live, to a first approximation, upper middle class lives in predominantly coastal cities. They still care about their kid&rsquo;s education and spend time thinking about it (sometimes more time than those bed nets).</p>
<p>My thesis: <strong>we are all maximizing our personal utility</strong>, it&rsquo;s just that one way of maximizing it is to <em>feel</em> like the work you&rsquo;re doing matters in some way&hellip;perhaps as part of a global movement to do the quantifiably right thing.</p>
<p>It&rsquo;s the old argument - are you helping grandma cross the street because <em>it</em> is good or because <em>it makes you feel good</em>? The latter is arguably a selfish endeavor.</p>
<h2 id="ea-dramatically-underestimates-the-value-of-maximizing-personal-utility">EA dramatically underestimates the value of maximizing personal utility</h2>
<p>Remember Dustin? He didn&rsquo;t get rich by working on EA in his early 20s. He had an upper-middle-class upbringing, went to Harvard, then started Facebook. Instead of working on charitable work upfront, he spent his time (and money) at a company building something interesting and valuable. He then went on to start Asana, <em>another</em> company that was interesting and valuable.</p>
<p>Dustin is maximizing global utility <em>now</em> because there&rsquo;s unlikely to be much left in terms of personal utility. That makes sense!</p>
<p>But he&rsquo;s surrounded by a much larger group of people who are contributing to maximizing global utility in favor of personal utility as part of the effective altruist movement. They are predominantly upper middle class, intellectual, and high potential&hellip;pretty much the same position that Dustin was in before he started his first company.</p>
<p>The difference is that instead of making a discontinuous, risky bet, these people are optimizing the charitable giving of people who&rsquo;ve already made that bet successfully. Or they&rsquo;re taking a pseudo-random percentage of their income and donating it every year. With all due respect for the value of what they&rsquo;re doing, I think that&rsquo;s a mistake.</p>
<h2 id="recommendation-for-readers-who-havent-collapsed-from-boredom">Recommendation for readers who haven&rsquo;t collapsed from boredom</h2>
<p>I hypothesize that there are roughly three categories of people relevant to EA:</p>
<ol>
<li>
<p>The rich people - these folks have completely diminishing personal marginal utility, so they should spend their money on things that have the highest impact. They should do EA, but remember that long-termist EA isn&rsquo;t measured quite the same way. Think killer robots are going to take over the world? Fine - spend money and fix that. Think it&rsquo;s a super virus? Asteroid? China&rsquo;s hegemony? Whatever you think is most important, there is probably a case to be made that EA is a fit. The one exception is if you want to change something specific in your local community right now, in which case you&rsquo;re arguably not making an EA investment, you&rsquo;re making one of personal utility (which I think is still commendable, though pure EAs would not agree).</p>
</li>
<li>
<p>The intellectuals - these folks are the brilliant thinkers and contributors that are part of the EA and/or rationalist movement. They have a strong moral compass, a good sense of how to weigh things of quantitative value, and (as good as anyone) the ability to ballpark qualitative value. They can&rsquo;t compare the two that well. They can&rsquo;t sum personal utility with the global utility to see why a local food bank might sometimes be a good interventional choice.</p>
</li>
<li>
<p>The normies - these people are working hard to get to the next level. While charity from them is admirable, their personal utility is so much larger than it is for the other groups that the general recommendation should be to not participate in EA, at least financially. Yes, you should pay off your student loans (listen to Dave Ramsey). Many people who think they are in category (2) are really in this category.</p>
</li>
</ol>
<h1 id="go-do-something-interesting-and-valuable">Go do something interesting and valuable.</h1>
<p>My main message is to the folks in category 2. The standard effective altruism expected value calculation allows for uncertainty and hedging of risk across interventions but doesn&rsquo;t bother to translate those to an individual person&rsquo;s level.</p>
<p>If what is ultimately a pretty small amount of money to the world but a large amount of money to you will help you start that next idea, build the next company, write the next book, create the next organization, start the next agency, etc, you should do it.</p>
<p>Ignore the pocket calculators in the corner calculating how many (in expectation) QALYs are now somehow your fault. They aren&rsquo;t.</p>
<p>You&rsquo;re doing exactly what the financial backers of the EA movement themselves did: looking at investments to maximize <em>a combination of personal utility and global utility</em>. The only difference is that the first variable goes to zero as you get richer.</p>
<p>And if it really is the case that you&rsquo;re at diminishing returns for personal utility, then you slip closer to category 1 and should consider EA as a way of optimizing a global contribution. I think EA&rsquo;s primary mistake is conflating everyone to be in the same category when they&rsquo;re obviously not.</p>
<hr>
<p>[1] <a href="https://www.effectivealtruism.org/articles/introduction-to-effective-altruism">https://www.effectivealtruism.org/articles/introduction-to-effective-altruism</a></p>
<p>[2] The EA forum has a red team contest for criticisms of EA. I was already planning to write a post like this, but the financial incentive helped get it out the door. <a href="https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#How_to_apply">https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming#How_to_apply</a></p>
<p>[3] <a href="http://www.paulgraham.com/say.html">http://www.paulgraham.com/say.html</a></p>
<p>[4] <a href="https://astralcodexten.substack.com/p/effective-altruism-as-a-tower-of">https://astralcodexten.substack.com/p/effective-altruism-as-a-tower-of</a></p>
<p>[5] <a href="https://www.effectivealtruism.org/articles/cause-profile-animal-welfare">https://www.effectivealtruism.org/articles/cause-profile-animal-welfare</a></p>
<p>[6] <a href="https://michaelnotebook.com/eanotes">https://michaelnotebook.com/eanotes</a></p>
<p>[7] <a href="https://www.openphilanthropy.org/research/hits-based-giving/">https://www.openphilanthropy.org/research/hits-based-giving/</a></p>
<p>[8] <a href="https://80000hours.org/2021/08/effective-altruism-allocation-resources-cause-areas/">https://80000hours.org/2021/08/effective-altruism-allocation-resources-cause-areas/</a></p>
<p>[9] The best quote in the Drake equation wikipedia article is the following: &ldquo;Criticism of the Drake equation follows mostly from the observation that several terms in the equation are largely or entirely based on conjecture. Star formation rates are well-known, and the incidence of planets has a sound theoretical and observational basis, but the other terms in the equation become very speculative. The uncertainties revolve around our understanding of the evolution of life, intelligence, and civilization, not physics. No statistical estimates are possible for some of the parameters, where only one example is known. The net result is that the equation cannot be used to draw firm conclusions of any kind, and the resulting margin of error is huge, far beyond what some consider acceptable or meaningful.&rdquo;</p>
<p>[10] For other good criticisms of EA, see: <a href="https://whyphilanthropymatters.com/article/why-am-i-not-an-effective-altruist/">https://whyphilanthropymatters.com/article/why-am-i-not-an-effective-altruist/</a>
<a href="https://forum.effectivealtruism.org/posts/xBBXf7KXZCKHYBxeZ/patrick-collison-on-effective-altruism">https://forum.effectivealtruism.org/posts/xBBXf7KXZCKHYBxeZ/patrick-collison-on-effective-altruism</a>
<a href="https://freddiedeboer.substack.com/p/effective-altruism-has-a-novelty">https://freddiedeboer.substack.com/p/effective-altruism-has-a-novelty</a>
<a href="https://www.abc.net.au/religion/why-effective-altruism-is-not-effective/13310708">https://www.abc.net.au/religion/why-effective-altruism-is-not-effective/13310708</a></p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Sanjay Saraf </span>
                </p>
            
           
            <p class="copyright-item">
                    <span>Words:</span>
                   <span>2949</span>
            </p>
            
            <p class="copyright-item">
                
            </p>

             
            <p class="copyright-item">
                Released under <a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a>
            </p>
            
    </div>

  
    <div class="post-tags">
        
            <section>
            Tag: 
            
            <span class="tag"><a href="https://ssaraf.com/tags/writing/">
                    #writing</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">Back</a></span> · 
                <span><a href="https://ssaraf.com">Home</a></span>
        </section>
    </div>
<br>
    <div class="post-nav">
        
        <a href="https://ssaraf.com/bandwidth/" class="prev" rel="prev" title="Bandwidth of communication"><i class="iconfont icon-dajiantou"></i>&nbsp;Previous: Bandwidth of communication</a>
         
        
    </div>

    <div class="post-comment">
          
          

 

          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2023</span>
        
         
            <span class="author" itemprop="copyrightHolder"><a href="https://ssaraf.com">Sanjay Saraf</a> | </span>
         

		  <span>Crafted with ❤️ by <a href="https://github.com/Fastbyte01/KeepIt" target="_blank" rel="external nofollow noopener noreffer">KeepIt</a> & <a href="https://gohugo.io/" target="_blank" rel="external nofollow noopener noreffer">Hugo</a></span>
    </div>
</footer>












    
    <link crossorigin="anonymous" integrity="sha384-yziQACfvCVwLqVFLqkWBYRO3XeA4EqzfXKGwaWnenYn5XzqfJFlFdKEmvutIQdKb" href="https://lib.baomitu.com/lightgallery/1.10.0/css/lightgallery.min.css" rel="stylesheet">
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  







     </div>
  </body>
</html>
